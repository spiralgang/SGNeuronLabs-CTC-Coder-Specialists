import qai_hub as hub
import torch
from torchvision.models import mobilenet_v2
import requests
import numpy as np
from PIL import Image

# 1. Load pre-trained MobileNetV2 model
torch_model = mobilenet_v2(pretrained=True)
torch_model.eval()

# 2. Trace the model for TorchScript export
input_shape = (1, 3, 224, 224)
example_input = torch.rand(input_shape)
traced_torch_model = torch.jit.trace(torch_model, example_input)

# 3. Compile model for a specific device (Samsung Galaxy S24 Family)
compile_job = hub.submit_compile_job(
    model=traced_torch_model,
    device=hub.Device("Samsung Galaxy S24 (Family)"),
    input_specs=dict(image=input_shape),
)
compile_job.wait()  # Block until complete (recommended for production)

# 4. Profile model on a real hosted device
target_model = compile_job.get_target_model()
profile_job = hub.submit_profile_job(
    model=target_model,
    device=hub.Device("Samsung Galaxy S24 (Family)"),
)
profile_job.wait()  # Optional: block for profiling completion

# 5. Prepare and preprocess a sample image
sample_image_url = (
    "https://qaihub-public-assets.s3.us-west-2.amazonaws.com/apidoc/input_image1.jpg"
)
response = requests.get(sample_image_url, stream=True)
response.raw.decode_content = True
image = Image.open(response.raw).resize((224, 224))
input_array = np.expand_dims(
    np.transpose(np.array(image, dtype=np.float32) / 255.0, (2, 0, 1)), axis=0
)

# 6. Run inference on the cloud-hosted device
inference_job = hub.submit_inference_job(
    model=target_model,
    device=hub.Device("Samsung Galaxy S24 (Family)"),
    inputs=dict(image=[input_array]),
)
inference_job.wait()  # Block until inference completes
on_device_output = inference_job.download_output_data()

# 7. Post-process output: softmax and top-5 predictions
output_name = list(on_device_output.keys())[0]
out = on_device_output[output_name][0]
on_device_probabilities = np.exp(out) / np.sum(np.exp(out), axis=1)

# Fetch ImageNet class labels
sample_classes = "https://qaihub-public-assets.s3.us-west-2.amazonaws.com/apidoc/imagenet_classes.txt"
response = requests.get(sample_classes)
categories = [s.strip() for s in response.text.splitlines()]

# Print top-5 predictions
print("Top-5 On-Device predictions:")
top5_classes = np.argsort(on_device_probabilities[0])[-5:]
for c in reversed(top5_classes):
    print(f"{c} {categories[c]:20s} {on_device_probabilities[0][c]:>6.1%}")

# 8. Download optimized model
target_model.download("mobilenet_v2.tflite")

# References:
# - Vault: /reference (PyTorch, NumPy, PIL, Qualcomm AI Hub, Python packaging)
# - https://aihub.qualcomm.com/docs
